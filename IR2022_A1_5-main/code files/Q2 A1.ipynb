{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GSnhH09ZCtC",
        "outputId": "dc292872-4017-46fc-feed-3e3be9660ef6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlmZGTVqZBsP"
      },
      "source": [
        "## Q2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dcc_zTNbZnQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzJwrh4CZBsP"
      },
      "source": [
        "### IMPORTING IMPORTANT LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CY3-Zg_yZBsQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pprQlWbbZBsQ",
        "outputId": "1ce8aee9-77d4-4115-d344-31ba8d719b66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "##Stopwords\n",
        "import nltk  # import nltk for processing the text\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords # import the stopwords from nltk\n",
        "stop_words = set(stopwords.words('english')) # we are settting the stopwords to be english\n",
        "nltk.download('punkt') # download the punctuations which will be removed from the text later "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTsPsGZWZBsQ",
        "outputId": "e1fdb159-b070-4a5b-9b28-913541783e9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer # Lemmatize the text \n",
        "nltk.download('wordnet') # download the word net libarary from nltk\n",
        "# use the inbuilt word net lemamtizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-eRndiuZBsR"
      },
      "source": [
        "### DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HABexaBUZBsR"
      },
      "outputs": [],
      "source": [
        "## Opening the files\n",
        "directory_r = [f for f in os.listdir('/content/drive/MyDrive/') if not f.startswith('.')]\n",
        "directory = []\n",
        "for in_die in directory_r : # stores the complete path in directory where the data is stored.\n",
        "    \n",
        "    if in_die == 'data':\n",
        "        directory += [in_die]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX8tVzyNZBsR"
      },
      "source": [
        "### READING DATA THROUGH FILES\n",
        "#### PERFORMING POSITIONAL INDEXING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNf4bFejZBsR"
      },
      "outputs": [],
      "source": [
        "vocab = {}\n",
        "words = []\n",
        "for i in range(len(directory)):\n",
        "    files = os.listdir('/content/drive/MyDrive/' + directory[i])\n",
        "    \n",
        "    for j in range(len(files)):\n",
        "        \n",
        "        path = '/content/drive/MyDrive/' + directory[i] + '/' + files[j] # checks the number of files in the directory\n",
        "        \n",
        "        doc_id = files[j]\n",
        "        \n",
        "        word_position = 0\n",
        "        words = []\n",
        "        \n",
        "        text = open(path,'r',errors='ignore').read() # read the text from each files\n",
        "        text = text.lower()                                  #lower the text\n",
        "      \n",
        "        for p in \"!.,:@#$%^&?<>*()[}{]-=;/\\\"\\\\\\t\\n\":         # remove punctuations\n",
        "            if p in '\\n;?:!.,.':\n",
        "                \n",
        "                text = text.replace(p,' ')                   # remoce blank space \n",
        "            else:\n",
        "                \n",
        "                text = text.replace(p,'')\n",
        "        \n",
        "        tokens = nltk.word_tokenize(text)                    #tokenize the text\n",
        "        for item in tokens: \n",
        "            after_lemmatization = lemmatizer.lemmatize(item) #do lemmatization\n",
        "            if 'a'=='a': \n",
        "                if after_lemmatization not in stop_words :  # check for stopwords and remove them if found.\n",
        "                    words += [after_lemmatization]  \n",
        "                each_word = after_lemmatization                \n",
        "                vocab_keys = vocab.keys()\n",
        "                \n",
        "                if each_word in vocab_keys :                # the list of vocab taken from the text has been generated \n",
        "                    \n",
        "                    dictionary_inside = vocab[each_word]\n",
        "                    dictionary_inside_keys = dictionary_inside.keys()\n",
        "                    \n",
        "                    if doc_id in dictionary_inside_keys:    # if the word is already present in the vocl list tehn check position and append\n",
        "                         # Check if the term has existed in that DocID before.\n",
        "                        dictionary_inside_list = dictionary_inside[doc_id]   # for each word in the vocab list generated the position of the token in the documnet is checked.\n",
        "                        dictionary_inside_list.append(word_position)        # the positions of the tokens for each document have been appended here. \n",
        "                        \n",
        "                        vocab[each_word][doc_id] = dictionary_inside_list  # stores the word and position of it in the document.\n",
        "                    \n",
        "                    else:                         #if doc id is not there  create a list and append the word_position.\n",
        "                        dictionary_inside_list = []\n",
        "                        dictionary_inside_list.append(word_position)\n",
        "                        \n",
        "                        vocab[each_word][doc_id] = dictionary_inside_list # take the position of the word in the corresponding document.\n",
        "                    \n",
        "                 # If term does not exist in the positional index dictionary\n",
        "                 # (first encounter).    \n",
        "                else:   #if word not in vocab tokens\n",
        "                    inside_dictionary = {}\n",
        "                    \n",
        "                    inside_list = []\n",
        "                    inside_list.append(word_position)\n",
        "                    \n",
        "                    inside_dictionary[doc_id] = inside_list\n",
        "                    \n",
        "                    vocab[each_word] = inside_dictionary\n",
        "            \n",
        "                word_position = word_position + 1\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTIHg4GVZBsR",
        "outputId": "72104c36-e59d-4227-c5e0-ccce4f22dd38"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1879"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Length of the vocab created from above.\n",
        "len(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "la8FK0VkZBsS"
      },
      "source": [
        "### Phrase Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VS3dR3LZBsS",
        "outputId": "6e71d573-d8f4-4951-fc20-da5e6d4ca5fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the phrase query:  Well Costello, I'm going\n"
          ]
        }
      ],
      "source": [
        "# phrase = \"use for minor cuts and grazes\"\n",
        "#Enter the phrase query to be checked in the text\n",
        "try:\n",
        "    phrase = input(\"Enter the phrase query:  \")\n",
        "except:\n",
        "    print(\"Exception occured. \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8chJo6_ZBsS"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HlK0WfbZBsS"
      },
      "outputs": [],
      "source": [
        "# we got the tokens of the query and we have assumed if the length of the tokens are greater than 5 then we will print the given statement.\n",
        "if(len(phrase)<=5)\n",
        "phrase = phrase.lower()\n",
        "\n",
        "for p in \"!.,:@#$%^&?<>*()[}{]-=;/\\\"\\\\\\t\\n\":         # remove punctuations\n",
        "            if p in '\\n;?:!.,.':\n",
        "                \n",
        "                phrase = phrase.replace(p,' ')                   # remove blank space \n",
        "            else:\n",
        "                \n",
        "                phrase = phrase.replace(p,'')\n",
        "                \n",
        "tokens = nltk.word_tokenize(phrase) \n",
        "tokens_without_sw = [word for word in tokens if not word in stopwords.words()]\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3Evf6nDZBsS",
        "outputId": "25bdfcab-c977-45e0-d0fa-aa5899dd3b3c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['well', 'costello', \"'m\", 'going']"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens_without_sw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gh4OjGdsZBsS",
        "outputId": "cdc06df9-07e3-46f3-f3ac-cbddf57f67a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['well', 'costello', \"'m\", 'going']\n"
          ]
        }
      ],
      "source": [
        "# perform preprocessing in the query.\n",
        "if(len(tokens_without_sw)>=5):\n",
        "    print(\"Phrase Query of length greater than one.\")\n",
        "else:    \n",
        "    phrase_lemmat = []\n",
        "    for item in tokens_without_sw:\n",
        "        item = item.lower()\n",
        "        phrase_lemmat += [lemmatizer.lemmatize(item)]\n",
        "        \n",
        "    print(phrase_lemmat)\n",
        "    # print the preprocessed query.\n",
        "    result = {}\n",
        "    count = 0\n",
        "\n",
        "        # from the token generated by the query, search the documents and prints the documents were the query is present.\n",
        "    for item in phrase_lemmat : \n",
        "        try:\n",
        "            inside_dictionary = vocab[item]\n",
        "        except:\n",
        "            print(\"Word Not Present\")\n",
        "\n",
        "\n",
        "        if len(result) == 0 :\n",
        "            result = inside_dictionary\n",
        "        else:\n",
        "            doc_list = list(set(result.keys()) & set(inside_dictionary.keys()))\n",
        "            temp_dic = {}\n",
        "\n",
        "            for doc in doc_list :\n",
        "                temp_dic[doc] = result[doc]    \n",
        "            result = {}    \n",
        "            result = temp_dic\n",
        "            if len(doc_list) == 0:     # if no matching of the token, the print \"NO documents\" present\n",
        "                print(\"No documents\")\n",
        "                break\n",
        "\n",
        "            for doc in doc_list :    # if the token is present and found in any documents, \n",
        "                                     #append the position, extract the document and print it.\n",
        "\n",
        "                list_posting = result[doc]\n",
        "                temp_pos = []\n",
        "\n",
        "                for pos in list_posting:\n",
        "\n",
        "                    if pos+count in inside_dictionary[doc]:# checks for each document, in how many documents the tokens are present.\n",
        "                        temp_pos.append(pos)\n",
        "\n",
        "                result[doc] = temp_pos\n",
        "\n",
        "                if len(result[doc]) == 0 :\n",
        "                    del result[doc]\n",
        "\n",
        "        count = count + 1     # increment the count of the documents in which we found the tokens.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKyR2b6EZBsT"
      },
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TWGfXAVZBsT",
        "outputId": "690c6200-e7cc-4c12-efc0-3c74ec87bce1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'abbott.txt': [11], 'aclamt.txt': [168], 'a_fish_c.apo': [263]}"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result  # print the result, prints the document name and the position at which the token is present."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDCmALrcZBsT",
        "outputId": "fc6e65d5-7913-404d-ab2e-09b319aaa22a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Documents retrived are : 3\n",
            "List of Documents retrived are : {'abbott.txt': [11], 'aclamt.txt': [168], 'a_fish_c.apo': [263]}\n"
          ]
        }
      ],
      "source": [
        "# Length of documents retrived\n",
        "\n",
        "print(\"Number of Documents retrived are :\" ,len(result)) \n",
        "## The Documents retrived with the query\n",
        "\n",
        "print(\"List of Documents retrived are :\",result)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Q2.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

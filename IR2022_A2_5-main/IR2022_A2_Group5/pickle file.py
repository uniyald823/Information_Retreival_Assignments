# -*- coding: utf-8 -*-
"""Copy of Pickle file.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VzeJGPRsUOlQhz1SGa3sqFBUiBZAt3Uw

# Importing Required Python Modules
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import glob  
from tqdm import tqdm
from nltk import tokenize
import nltk
import re
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk import word_tokenize
from nltk.stem import WordNetLemmatizer 
import operator
import os
import pandas as pd
import pickle
import string
from collections import Counter
import inflect
import nltk
import numpy as np
from nltk import tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

#Considering the punctuations 
print(string.punctuation)
stop_words = set(stopwords.words('english'))

"""### Preprocessing of Data


1.   Strip the data for each files 
2.   Convert into lower case
3.   Tokenize the data
4.   Converting the number into word like 12 -> twelve
5.   Remove punctuation
6.   Removing single alphabet
7.   Remove stop words
8.   Apply lemmetization
"""

def preprocessing(tokens):
    token = [word.lower() for word in tokens]
    curren_words = []

    for word in token:
        if word.isdigit():
            try:
                curr_words = p.number_to_words(word)
                curren_words.append(curr_words)
            except:
                curren_words.append(word)
        else:
            curren_words.append(word)
    punctuations = str.maketrans('  ', '  ', string.punctuation)
    remove_punctuation = [w.translate(punctuations) for w in curren_words]
    remove_stopwords = [word for word in remove_punctuation if word.isalpha()]
    token = [w for w in remove_stopwords if not w in stop_words]
    lemmatizer = nltk.stem.WordNetLemmatizer()
    s = PorterStemmer()
    token = [s.stem(word) for word in token]
    return token

def file_reading(path):
    i = 0
    main_files = ['comp.graphics',"sci.med", "talk.politics.misc","rec.sport.hockey","sci.space"]
    path1 = "/content/drive/MyDrive/Assignment 3/20_newsgroups"
    paths = []
    la = []
    files_paths = glob.glob(path)
    for f in tqdm(files_paths,leave=True,position=0,desc ="File Processed Till now"):
        file_name,file_tail = os.path.split(f)
        for i in main_files:
          if(file_tail == i):
            for file in glob.glob(f+"/*"):
                file_name1,file_tail1 = os.path.split(file)
                paths.append(str(path1)+"/"+file_tail+"/"+file_tail1)
                la.append(file_tail)
    return paths,la

dirpath = "/content/drive/MyDrive/Assignment 3/20_newsgroups/*"

path,names = file_reading(dirpath)

document_iterator=0
list_in_docs = []
for p in tqdm(path):
  txt_data = open(p,mode='r',errors='ignore', encoding='UTF8').read().strip()
  create_tokens = tokenize.RegexpTokenizer(r'\w+').tokenize(txt_data)
  tokenized_data = preprocessing(create_tokens)
  list_in_docs.append(tokenized_data)
  document_iterator += 1
data = pd.DataFrame([list_in_docs, names]).T

"""[Ref for dumping in pickel ](https://stackoverflow.com/questions/11218477/how-can-i-use-pickle-to-save-a-dict)"""

# with open('/content/drive/MyDrive/Assignment 3/Doc_pdAssignment3.pickle', 'wb') as handle:
#     pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)
with open('Doc_pdAssignment3.pickle', 'wb') as handle:
    pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)

data

